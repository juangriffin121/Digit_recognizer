{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b19608e",
   "metadata": {},
   "source": [
    "$\\vec{\\nabla}\\downarrow$\n",
    "\n",
    "Neural networks are universal function aproximators, for any kind of prediciton problem where the data follows a very complex but not random functional correspondance between input and output, neural networks are a usefull tool. \n",
    "\n",
    "When we have a dataset with entries of the form $(X,Y_{true})$\n",
    "\n",
    "and the data follows some complex and unknowable functional relation $Y = F(X) + \\epsilon $ with certain error due to variables not taken into account in the data or errors in the labeling process\n",
    "\n",
    "The trained neural network as a black box works like this:\n",
    "\n",
    "$Y = N(X,\\Theta) = N_\\Theta(X)$\n",
    "\n",
    "It comes from a function of an imput and parameters that when fitted to the data it best predicts the real output from the input. It takes an input X which has to be an n dimensional array of numerical values (for non numerical data there are ways of converting it into numerical form), and it has a set $\\Theta$ of fitted optimizable numerical parameters which define the prediction of the network to any posible input, and it returns another array Y as an output.\n",
    "\n",
    "N in this case is a known function whos output from a certain input is determined by the parameters, and with enough complexity and optimized parameters they can fit any functional form between the data\n",
    "\n",
    "Gradient decent is a process of optimization of parameters to fit the dataset on which the network will be trained\n",
    "\n",
    "The dataset D is a list of datapoints of the shape $(X,Y_{true})$\n",
    "\n",
    "It uses a loss or cost function that computes how bad the prediction is for either a single data point or a subset in the case of stochastic gradient descent, or the entire dataset, it ends up being a function of the parameters\n",
    "\n",
    "$E(\\Theta)$  a common example is mean squares $E(\\Theta) = \\sum \\limits _{i = 0} ^{n} (Y_{true,i} - F_\\Theta(X_i))^2$\n",
    "\n",
    "\n",
    "It calculates the derivative or the \"gradient\" of the loss with respect to the parameters \n",
    "\n",
    "$\\frac{dE}{d\\Theta}$ also written as $\\nabla E$ since the first notation is more explicit we'll use it \n",
    "\n",
    "And since moving in the direction of the negative gradient is the direction of greatest decrease of the function the parameters are modified moving a small step in the direction of their derivatives:\n",
    "\n",
    "$\\Theta = \\Theta - \\lambda \\frac{dE}{d\\Theta}$\n",
    "\n",
    "The optimization process is done in various iterations until the parameters fit the data well enough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb215e15",
   "metadata": {},
   "source": [
    "The particular functional form of the network and the way the parameters affect the output depends on the type of network used which may differ depending on the problem thats needed to solve\n",
    "\n",
    "A general property that machine learning algorithms that use neural network have is they are composed of layers, each layer has its own input and output and can have a subset of the parameters that define its output from the input, and they are concatenated in a way that the output of one is the input of the next one, so that when the input enters the first layer it runs through the network and returns an output out the last layer \n",
    "\n",
    "$\\textbf{y}_i =  l(\\textbf{x}_i,\\theta) = l_\\theta(\\textbf{x}_i)$\n",
    "\n",
    "$\\textbf{x}_{i+1} = \\textbf{y}_i$\n",
    "\n",
    "$\\textbf{x}_0 = X$\n",
    "\n",
    "$\\textbf{x}_n = Y$\n",
    "\n",
    "The layers allow the algorithm to have enough complexity to properly fit potentialy complex data but has components that are simpler to deal with and there are many well defined types of layers which we'll talk about that are better suited for different purposes and they can be mixed in the network\n",
    "\n",
    "The layered structure also helps in calculating the derivatives, if the functional form of the particular layer has a simple formula for its derivative with respect to its parameters the derivative of the loss can be easily calculated with the chain rule\n",
    "\n",
    "$\\frac{dE}{d\\theta} = \\frac{dE}{d\\textbf{y}} \\frac{d\\textbf{y}}{d\\theta} = \\frac{dE}{d\\textbf{y}} \\frac{\\partial l}{\\partial \\theta}(\\textbf{x},\\theta)$\n",
    "\n",
    "and to get each $\\frac{dE}{dy}$ for each layer we can calculate with the chain rule $\\frac{dE}{dx}$ which will be the new gradient needed for the previous layer in a process called backpropagation\n",
    "\n",
    "$\\frac{dE}{d\\textbf{x}} = \\frac{dE}{d\\textbf{y}} \\frac{d\\textbf{y}}{d\\textbf{x}} = \\frac{dE}{d\\textbf{y}} \\frac{\\partial l}{\\partial \\textbf {x}}(\\textbf{x},\\theta)$\n",
    "\n",
    "The first $\\frac{dE}{dy}$ comes from the output of the entire network $Y$ and the $\\frac{dE}{dY}$ should be obtainable from the formula for $E(Y_{true},Y)$\n",
    "\n",
    "Therefore along with the explicit formula for $l$, in order to be able to optimize the layer we need to be able to get an explicit formula for $\\frac{\\partial l}{\\partial x}(\\textbf{x},\\theta)$ and $\\frac{\\partial l}{\\partial \\theta}(\\textbf{x},\\theta)$, having those for every layer, together with the formula for the derivative of the error with respect to the predicted output of the network $\\frac{dE}{dY}$, we are able to perform the gradient descent method to fit the full network to the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa3bd9b",
   "metadata": {},
   "source": [
    "The neural network in this project is coded as an ordered list of layer objects\n",
    "\n",
    "A layer is an object that contains two methods, forward and backward\n",
    "\n",
    "Forward calculates the output to the layer from the input, both the output and input are ndarrays, the shape of both is inputed in the initialization of the object and the shapes they can accepts depend on the type of layer.\n",
    "\n",
    "$\\textbf{y} = l(\\textbf{x},\\theta) = l_{\\theta}(\\textbf{x})$\n",
    "\n",
    "\n",
    "\n",
    "Backward calculates the gradient of the loss function with respect to the input as a function of gradient of the loss function with respect to the output with a contraction product between $\\frac{dE}{dY}$ and $\\frac{dY}{d\\textbf{x}}$ that follows the chain rule for the particular shape of the arrays\n",
    "\n",
    "$\\frac{dE}{d\\textbf{x}} = \\frac{dE}{d\\textbf{y}} \\frac{d\\textbf{y}}{d\\textbf{x}} = \\frac{dE}{d\\textbf{y}} \\frac{\\partial l}{\\partial \\textbf {x}}(\\textbf{x},\\theta)$\n",
    "\n",
    "the contraction product formula is this where $I$ is an index array like (i,j,k) for a 3D array\n",
    "\n",
    "$\\frac{dE}{d\\textbf{y}} \\cdot \\frac{d\\textbf{y}}{d\\textbf{x}} = \\sum \\limits_{I = \\vec{0}}^{Yshape}(\\frac{dE}{d\\textbf{y}_I}\\frac{d\\textbf{y}_I}{d\\textbf{x}})$\n",
    "\n",
    "here we have derivatives of a simple float with respect to an ndarray, its going to be an ndarray with the same shape as the original array, and derivatives of an ndarray with respect to another ndarray, this will result in a Y-shaped array of X-shaped arrays of the sort $\\frac{dY_{I}}{dX}$.\n",
    "\n",
    "The resulting shape would be (*shape(Y), *shape(X))\n",
    "\n",
    "If the layer has any parameters the gradient of the loss function with respect to the parameter array as a function of gradient of the loss function with respect to the output and modifies the parameters following gradient descent\n",
    "\n",
    "$\\frac{dE}{d\\theta} = \\frac{dE}{d\\textbf{y}} \\frac{d\\textbf{y}}{d\\theta} = \\frac{dE}{d\\textbf{y}} \\frac{\\partial l}{\\partial \\theta}(\\textbf{x},\\theta)$\n",
    "\n",
    "$\\theta  = \\theta - \\lambda\\frac{dE}{d\\theta}$\n",
    "\n",
    "Then theres the function predict that takes in a network and computes the response of the network with the parameters it has at the moment from an input, it runs through the network in order and uses the output of the previous layer as the input to the previous, this is the function that is going to be used once the network is trained.\n",
    "\n",
    "predict(Net, In)\n",
    "\n",
    "The function train takes the network and the training dataset, it runs through the dataset, and with each data entry, it runs through the network forward calling the forward method to get all the activation values for each layer, and it the runs the network backwards calling the backward method to fit the parameters, this done in a number of iterations  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1df7251",
   "metadata": {},
   "source": [
    "### Dense layer\n",
    "\n",
    "For the dense layer the function $l$ calculates the vector output $y$ is a linear combination of its vector imput $x$ with a vector bias\n",
    "\n",
    "$y_j = \\sum \\limits _{i = 0} ^{N} w_{ji}\\cdot x_i + b_j$\n",
    "\n",
    "$\\theta = (W,\\vec{b})$\n",
    "\n",
    "$\\vec{y} = l(\\vec{x},(W,\\vec{b})) = W\\vec{x} + \\vec{b}$\n",
    "___\n",
    "and the two needed derivatives $\\frac{\\partial l}{\\partial x}(\\textbf{x},\\theta)$ and $\\frac{\\partial l}{\\partial \\theta}(\\textbf{x},\\theta)$:\n",
    "\n",
    "$\\frac{\\partial l}{\\partial x}(\\textbf{x},\\theta)$:\n",
    "\n",
    "$\\frac{\\partial y_j}{\\partial x_i}(\\textbf{x},\\theta) \n",
    "= \\frac{\\partial}{\\partial x_i}(\\sum \\limits _{k = 0} ^{N} w_{jk}\\cdot x_k + b_j)\n",
    "=\\sum \\limits _{k = 0} ^{N} w_{jk}\\cdot \\frac{\\partial x_k}{\\partial x_i}\n",
    "=\\sum \\limits _{k = 0} ^{N} w_{jk}\\cdot \\delta_{ki}\n",
    "=w_{ji}\n",
    "$\n",
    "\n",
    "The object $\\delta_{ki}$ is called the kroneker delta in tensor calculus and its the entrys of the identity matrix\n",
    "\n",
    "$\\frac{\\partial l}{\\partial x}(\\textbf{x},\\theta) = W$\n",
    "___\n",
    "$\\frac{\\partial l}{\\partial \\theta}(\\textbf{x},\\theta)$: ($\\frac{\\partial l}{\\partial W}(\\textbf{x},\\theta)$,$\\frac{\\partial l}{\\partial \\vec{b}}(\\textbf{x},\\theta)$)\n",
    "\n",
    "$\\frac{\\partial l}{\\partial \\vec{b}}(\\textbf{x},\\theta)$:\n",
    "\n",
    "$\\frac{\\partial y_j}{\\partial b_i}(\\textbf{x},\\theta)\n",
    "=\\frac{\\partial}{\\partial b_i}(\\sum \\limits _{k = 0} ^{N} w_{jk}\\cdot x_k + b_j)\n",
    "=\\frac{\\partial b_j}{\\partial b_i}\n",
    "= \\delta_{ji}\n",
    "$\n",
    "\n",
    "$\\frac{\\partial l}{\\partial \\vec{b}}(\\textbf{x},\\theta) = I$\n",
    "___\n",
    "$\\frac{\\partial l}{\\partial W}(\\textbf{x},\\theta)$:\n",
    "\n",
    "$\\frac{\\partial yj}{\\partial w_{ik}}(\\textbf{x},\\theta)\n",
    "=\\frac{\\partial}{\\partial w_{ik}}(\\sum \\limits _{u = 0} ^{N} w_{ju}\\cdot x_u + b_j)\n",
    "=(\\sum \\limits _{u = 0} ^{N} \\frac{\\partial w_{ju}}{\\partial w_{ik}} x_u)\n",
    "=(\\sum \\limits _{u = 0} ^{N} \\delta_{ij}\\delta_{uk}\\cdot x_u)\n",
    "=(\\delta_{ij} x_k)\n",
    "$\n",
    "\n",
    "this last object is a 3D array which can be thought of as being the result of multiplying each entry of the identity matrix by the input vector, we'll represent this with the tensor product\n",
    "\n",
    "$\\frac{\\partial l}{\\partial W}(\\textbf{x},\\theta) = I\\otimes \\vec{x}$\n",
    "___\n",
    "with those functions obtained we can find the needed gradients for the backward method\n",
    "\n",
    "$\\frac{dE}{d\\textbf{x}} = \\frac{dE}{d\\textbf{y}} \\frac{d\\textbf{y}}{d\\textbf{x}} \n",
    "= \\frac{dE}{d\\textbf{y}} \\frac{\\partial l}{\\partial \\textbf {x}}(\\textbf{x},\\theta) \n",
    "= (\\frac{dE}{d\\vec{y}})^T \\cdot W\n",
    "$\n",
    "\n",
    "$\\frac{dE}{d\\theta} = \\frac{dE}{d\\textbf{y}} \\frac{d\\textbf{y}}{d\\theta} = \\frac{dE}{d\\textbf{y}} \\frac{\\partial l}{\\partial \\theta}(\\textbf{x},\\theta)$\n",
    "\n",
    "$\\frac{\\partial E}{\\partial \\theta}$: ($\\frac{\\partial E}{\\partial W}$,$\\frac{\\partial E}{\\partial \\vec{b}}$)\n",
    "\n",
    "$\\frac{dE}{dW} = \\frac{dE}{d\\textbf{y}} \\frac{d\\textbf{y}}{dW} = \\frac{dE}{d\\textbf{y}} \\frac{\\partial l}{\\partial W}(\\textbf{x},\\theta) \n",
    "= (\\frac{dE}{d\\vec{y}})^T \\cdot (I \\otimes \\vec{x}) = (\\frac{dE}{d\\vec{y}})^T\\vec{x}\n",
    "$\n",
    "\n",
    "$\\frac{dE}{d\\vec{b}} = \\frac{dE}{d\\textbf{y}} \\frac{d\\textbf{y}}{d\\vec{b}} = \\frac{dE}{d\\textbf{y}} \\frac{\\partial l}{\\partial \\vec{b}}(\\textbf{x},\\theta) \n",
    "= \\frac{dE}{d\\vec{y}} \\cdot I = \\frac{dE}{d\\vec{y}}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34178c1b",
   "metadata": {},
   "source": [
    "### Convolutional layer\n",
    "\n",
    "In the convolutional layer both the input and output are 3D arrays that can be thought of as vectors of matrices or images, if the first layer of the network is convolutional usually the vector of images contains the three matrices of red, green and blue pixel values, in this project since its just black or white it only contains one matrix but it still is ordered in a 3D array to keep a consistent structure. In the output of the first and in deeper convolutional layers the entries of the vector of matrices still resemble the original image but do not represent the colors but rather diferent features the network picks up.\n",
    "\n",
    "The central operation in this layer is the convolution or crosscorrelation, which in the case of images takes in an image and a kernel(smaller 2D array) and outputs another image, the kernel is slide through the input image and the overlapping entries are multiplied together and summed, in a 2D analog of the dot product, and the result of that product is stored in the corresponding index of the output image\n",
    "\n",
    "$\\textbf{v} = k*\\textbf{u}$\n",
    "\n",
    "$v_{ij} = \\sum \\limits_{(a,b) = (0,0)}^{(KShape)} (u_{(i-{\\lfloor}\\frac{L}{2}{\\rfloor} + a,j-{\\lfloor}\\frac{L}{2}{\\rfloor} + b)} \\cdot k_{ab})$\n",
    "\n",
    "each image in the output vector of images is afected by every image in the input, and its a sum of the convolution of all the images with kernels specific for each input image and output image, the result is analog to a linear combination and a bias\n",
    "\n",
    "$\n",
    "\\textbf{y}_n = \\sum \\limits_{m} \\textbf{k}_{nm}*\\textbf{x}_m + \\textbf{b}_n\n",
    "$\n",
    "\n",
    "If we define an analog product $(\\cdot *)$ that operates on two matrices of images or kernels and makes the convolution between the images and kernels and adds them together\n",
    "\n",
    "$\n",
    "\\textbf{y} =  l( \\textbf{x},( \\textbf{K}, \\textbf{b})) = \\textbf{K} (\\cdot*) \\textbf{x} + \\textbf{b}\n",
    "$\n",
    "\n",
    "$\\theta = ( \\textbf{K} , \\textbf{b})$\n",
    "___\n",
    "\n",
    "index expansion of function $l$\n",
    "\n",
    "$y_{nij} =  \\sum \\limits_{m} [\\textbf{k}_{nm}*\\textbf{x}_m]_{ij} + b_{nij}\n",
    "=\\sum \\limits_{m} (\\sum \\limits_{(a,b) = (0,0)}^{(KShape)} (x_{m,(i-{\\lfloor}\\frac{L}{2}{\\rfloor} + a,j-{\\lfloor}\\frac{L}{2}{\\rfloor} + b)} \\cdot k_{nmab})) + b_{nij}\n",
    "$\n",
    "___\n",
    "$\\frac{\\partial E}{\\partial x}$:\n",
    "\n",
    "$\n",
    "\\frac{\\partial y_{nij}}{\\partial x_{luv}} \n",
    "= \\frac{\\partial }{\\partial x_{luv}}(\\sum \\limits_{m} (\\sum \\limits_{(a,b) = (0,0)}^{(KShape)} (x_{m,(i-{\\lfloor}\\frac{L}{2}{\\rfloor} + a,j-{\\lfloor}\\frac{L}{2}{\\rfloor} + b)} \\cdot k_{nmab})) + b_{nij})\n",
    "=\\sum \\limits_{m} (\\sum \\limits_{(a,b) = (0,0)}^{(KShape)} (\\frac{\\partial }{\\partial x_{luv}}x_{m,(i-{\\lfloor}\\frac{L}{2}{\\rfloor} + a,j-{\\lfloor}\\frac{L}{2}{\\rfloor} + b)} \\cdot k_{nmab}))\n",
    "=\\sum \\limits_{m} (\\sum \\limits_{(a,b) = (0,0)}^{(KShape)} ( \\delta_{ml} \\delta_{u,(i-{\\lfloor}\\frac{L}{2}{\\rfloor} + a)} \\delta_{v,j-{\\lfloor}\\frac{L}{2}{\\rfloor} + b)} k_{nmab}))\n",
    "=\\sum \\limits_{(a,b) = (0,0)}^{(KShape)} (\\delta_{u,(i-{\\lfloor}\\frac{L}{2}{\\rfloor} + a)} \\delta_{v,(j-{\\lfloor}\\frac{L}{2}{\\rfloor} + b)} k_{nlab})\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\partial y_{nij}}{\\partial x_{luv}} \n",
    "=\\sum \\limits_{(a,b) = (0,0)}^{(KShape)} (\\delta_{u,(i-{\\lfloor}\\frac{L}{2}{\\rfloor} + a)} \\delta_{v,(j-{\\lfloor}\\frac{L}{2}{\\rfloor} + b)} k_{nlab})\n",
    "$\n",
    "\n",
    "$U = (i-{\\lfloor}\\frac{L}{2}{\\rfloor} + a)$\n",
    "\n",
    "$V = (j-{\\lfloor}\\frac{L}{2}{\\rfloor} + b)$\n",
    "\n",
    "$a = U-i + {\\lfloor}\\frac{L}{2}{\\rfloor}$\n",
    "\n",
    "$b = V-j + {\\lfloor}\\frac{L}{2}{\\rfloor}$\n",
    "\n",
    "$\n",
    "\\frac{\\partial y_{nij}}{\\partial x_{luv}} \n",
    "=\\sum \\limits_{(a,b) = (0,0)}^{(KShape)} (\\delta_{u,U} \\delta_{v,V} k_{nlab})\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\partial y_{nij}}{\\partial x_{lUV}} \n",
    "=k_{nlab}\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\partial y_{nij}}{\\partial x_{l,U,V}} \n",
    "=k_{n,l,(U-i + {\\lfloor}\\frac{L}{2}{\\rfloor}),(V-j + {\\lfloor}\\frac{L}{2}{\\rfloor})}\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\partial E}{\\partial x_{l,U,V}}\n",
    "=\\sum \\limits_{n,i,j = (0,0,0)}^{shape(y)} (\\frac{\\partial E}{\\partial y_{nij}}\\frac{\\partial y_{nij}}{\\partial x_{l,U,V}} )\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\partial E}{\\partial x_{l,U,V}}\n",
    "=\\sum \\limits_{n = 0}^{feats} (\\sum \\limits_{i,j = (0,0)}^{ImShape}\\frac{\\partial E}{\\partial y_{nij}}\\frac{\\partial y_{nij}}{\\partial x_{l,U,V}} )\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\partial E}{\\partial x_{l,U,V}}\n",
    "=\\sum \\limits_{n = 0}^{feats} (\\sum \\limits_{i,j = (0,0)}^{ImShape}\\frac{\\partial E}{\\partial y_{nij}}k_{n,l,(U-i + {\\lfloor}\\frac{L}{2}{\\rfloor}),(V-j + {\\lfloor}\\frac{L}{2}{\\rfloor})} )\n",
    "$\n",
    "\n",
    "i and j are now convolutional indexes 90 degree rotated kernel\n",
    "\n",
    "$\n",
    "\\frac{\\partial E}{\\partial x_{l}}\n",
    "=\\sum \\limits_{n = 0}^{feats} (\\frac{\\partial E}{\\partial y_{n}} * k_{nl})\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\partial E}{\\partial \\textbf x}\n",
    "=\\frac{\\partial E}{\\partial \\textbf y} (\\cdot * ) \\textbf K\n",
    "$\n",
    "\n",
    "___\n",
    "$y_{nij}\n",
    "=\\sum \\limits_{m} (\\sum \\limits_{(a,b) = (0,0)}^{(KShape)} (x_{m,(i-{\\lfloor}\\frac{L}{2}{\\rfloor} + a,j-{\\lfloor}\\frac{L}{2}{\\rfloor} + b)} \\cdot k_{nmab})) + b_{nij}\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\partial y_{nij}}{\\partial k_{\\nu\\mu uv}}\n",
    "=\\frac{\\partial }{\\partial k_{\\nu\\mu uv}}(\n",
    "\\sum \\limits_{m} (\\sum \\limits_{(a,b) = (0,0)}^{(KShape)} (x_{m,(i-{\\lfloor}\\frac{L}{2}{\\rfloor} + a,j-{\\lfloor}\\frac{L}{2}{\\rfloor} + b)} \\cdot k_{nmab})) + b_{nij}\n",
    ")\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\partial y_{nij}}{\\partial k_{\\nu\\mu uv}}\n",
    "=\\sum \\limits_{m} (\\sum \\limits_{(a,b) = (0,0)}^{(KShape)} (x_{m,(i-{\\lfloor}\\frac{L}{2}{\\rfloor} + a,j-{\\lfloor}\\frac{L}{2}{\\rfloor} + b)} \\cdot \\frac{\\partial k_{nmab}}{\\partial k_{\\nu\\mu uv}}))\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\partial y_{nij}}{\\partial k_{\\nu\\mu uv}}\n",
    "=\\sum \\limits_{m} (\\sum \\limits_{(a,b) = (0,0)}^{(KShape)} (x_{m,(i-{\\lfloor}\\frac{L}{2}{\\rfloor} + a,j-{\\lfloor}\\frac{L}{2}{\\rfloor} + b)} \\delta_{\\nu n} \\delta_{\\mu m} \\delta_{u a} \\delta_{v b}))\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\partial y_{nij}}{\\partial k_{\\nu\\mu uv}}\n",
    "=\\delta_{\\nu n} x_{\\mu,(i-{\\lfloor}\\frac{L}{2}{\\rfloor} + u,j-{\\lfloor}\\frac{L}{2}{\\rfloor} + v)} \n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\partial E}{\\partial k_{\\nu\\mu uv}}\n",
    "= \\sum \\limits_{nij = (0,0,0)}^{shape(y)}(\\frac{\\partial E}{\\partial y_{nij}} \\frac{\\partial y_{nij}}{\\partial k_{\\nu\\mu uv}})\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\partial E}{\\partial k_{\\nu\\mu uv}}\n",
    "= \\sum \\limits_{nij = (0,0,0)}^{shape(y)}(\\frac{\\partial E}{\\partial y_{nij}} \\cdot \\delta_{\\nu n} \\cdot x_{\\mu,(i-{\\lfloor}\\frac{L}{2}{\\rfloor} + u,j-{\\lfloor}\\frac{L}{2}{\\rfloor} + v)})\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\partial E}{\\partial k_{\\nu\\mu uv}}\n",
    "= \\sum \\limits_{ij = (0,0)}^{ImShape}(\\frac{\\partial E}{\\partial y_{\\nu ij}} \\cdot x_{\\mu,(i-{\\lfloor}\\frac{L}{2}{\\rfloor} + u,j-{\\lfloor}\\frac{L}{2}{\\rfloor} + v)})\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\partial E}{\\partial k_{\\nu\\mu uv}}\n",
    "= \\sum \\limits_{ij = (0,0)}^{ImShape}(\\frac{\\partial E}{\\partial y_{\\nu ij}} \\cdot x_{\\mu,(i-{\\lfloor}\\frac{L}{2}{\\rfloor} + u,j-{\\lfloor}\\frac{L}{2}{\\rfloor} + v)})\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\partial E}{\\partial k_{\\nu\\mu}}\n",
    "= \\frac{\\partial E}{\\partial y_{\\nu}} * x_{\\mu}\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\partial E}{\\partial \\textbf K}\n",
    "= \\frac{\\partial E}{\\partial \\textbf y} (\\otimes *) \\textbf x\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56eae2b",
   "metadata": {},
   "source": [
    "Momentum\n",
    "Further proposals include the momentum method or the heavy ball method, which in ML context appeared in Rumelhart, Hinton and Williams' paper on backpropagation learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
